{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwaraganathan/DeepLearning/blob/main/Melanoma_assignment_Dwaraganathan_DK_16thAug_4PM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBb-zQnMOjkM"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbHVUdigO0e4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qToIWGlPtlA"
      },
      "source": [
        "This assignment uses a dataset of about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_Bzca3mO3M_"
      },
      "outputs": [],
      "source": [
        "# Defining the path for train and test images\n",
        "## Todo: Update the paths of the train and test dataset\n",
        "data_dir_train = pathlib.Path(\"/content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train\")\n",
        "data_dir_test = pathlib.Path('/content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfpz9rVFPyIC"
      },
      "outputs": [],
      "source": [
        "image_count_train = len(list(data_dir_train.glob('*/*.jpg')))\n",
        "print(image_count_train)\n",
        "image_count_test = len(list(data_dir_test.glob('*/*.jpg')))\n",
        "print(image_count_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rZDEsTJP1LB"
      },
      "source": [
        "### Load using keras.preprocessing\n",
        "Let's load these images off disk using the helpful image_dataset_from_directory utility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD_ET3rKP792"
      },
      "source": [
        "#### Create a dataset\n",
        "Define some parameters for the loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILpKtGVWP5Dc"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOgQY6ktQGW9"
      },
      "source": [
        "Use 80% of the images for training, and 20% for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gvN-BqGQAyG"
      },
      "outputs": [],
      "source": [
        "## Write your train dataset here\n",
        "## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n",
        "## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir_train,\n",
        "    seed=123,\n",
        "    validation_split= 0.2,\n",
        "    subset= 'training',\n",
        "    image_size=(img_height,img_width),\n",
        "    batch_size = batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-50rMMNQJFw"
      },
      "outputs": [],
      "source": [
        "## Write your validation dataset here\n",
        "## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory\n",
        "## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir_train,\n",
        "    seed=123,\n",
        "    validation_split= 0.2,\n",
        "    subset= 'validation',\n",
        "    image_size=(img_height,img_width),\n",
        "    batch_size = batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcNXl3DCQLue"
      },
      "outputs": [],
      "source": [
        "# List out all the classes of skin cancer and store them in a list. \n",
        "# You can find the class names in the class_names attribute on these datasets. \n",
        "# These correspond to the directory names in alphabetical order.\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIeBolDjQEJu"
      },
      "source": [
        "### Visualize the data\n",
        "##### Todo, create a code to visualize one instance of all the nine classes present in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIudQKjTQSpE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(9): \n",
        "  plt.subplot(3, 3, i + 1)\n",
        "  image = plt.imread(str(list(data_dir_train.glob(class_names[i]+'/*.jpg'))[1]))\n",
        "  plt.title(class_names[i])\n",
        "  plt.imshow(image)\n",
        "### your code goes here, you can use training or validation data to visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBO3OnnsQWxm"
      },
      "source": [
        "The image_batch is a tensor of the shape (32, 180, 180, 3). This is a batch of 32 images of shape 180x180x3 (the last dimension refers to color channels RGB). The label_batch is a tensor of the shape (32,), these are corresponding labels to the 32 images.\n",
        "\n",
        "Dataset.cache() keeps the images in memory after they're loaded off disk during the first epoch.\n",
        "\n",
        "Dataset.prefetch() overlaps data preprocessing and model execution while training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT_vIZXaQYHs"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm8T5p-aQdVW"
      },
      "source": [
        "### Create the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK1mxAK-QgD7"
      },
      "outputs": [],
      "source": [
        "### Your code goes here\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "num_classes = 9\n",
        "model = Sequential([\n",
        "                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n",
        "])\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu', input_shape = (180, 180, 32)))\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation = \"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dgx9v5NhQmrd"
      },
      "source": [
        "### Compile the model\n",
        "##### Choose an appropirate optimiser and loss function for model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hpsiq1PpQkLP"
      },
      "outputs": [],
      "source": [
        "### Todo, choose an appropirate optimiser and loss function\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap4EVQ0UQt_m"
      },
      "outputs": [],
      "source": [
        "# View the summary of all layers\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btok02S9Q5bG"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q91ZpWmqQ4Ct"
      },
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjXkD8vuRBgc"
      },
      "source": [
        "### Visualizing training results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFWcFIbZRHwl"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWsB6uusReGE"
      },
      "source": [
        "### Todo: Write your findings after the model fit, see if there is an evidence of model overfit or underfit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t657ikLIRkDE"
      },
      "source": [
        "Finding on the first base model\n",
        "\n",
        "The model is overfitting because we can also see difference in loss functions in training & test around the 10-11th epoch\n",
        "\n",
        "The accuracy is just around 75-80% because there are enough features to remember the pattern.\n",
        "\n",
        "But again, it's too early to comment on the overfitting & underfitting debate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsJhK4nnRiBx"
      },
      "outputs": [],
      "source": [
        "# Todo, after you have analysed the model fit history for presence of underfit or overfit, choose an appropriate data augumentation strategy. \n",
        "data_augument = keras.Sequential([\n",
        "                             layers.experimental.preprocessing.RandomFlip(mode=\"horizontal_and_vertical\",input_shape=(img_height,img_width,3)),\n",
        "                             layers.experimental.preprocessing.RandomRotation(0.2, fill_mode='reflect'),\n",
        "                             layers.experimental.preprocessing.RandomZoom(height_factor=(0.2, 0.3), width_factor=(0.2, 0.3), fill_mode='reflect')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTfcNlJfRvsL"
      },
      "outputs": [],
      "source": [
        "# Todo, visualize how your augmentation strategy works for one instance of training image.\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(data_augument(images)[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0bD2YGfRyer"
      },
      "source": [
        "### Todo:\n",
        "Create the model, compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjnDaUG8R0KK"
      },
      "outputs": [],
      "source": [
        "## You can use Dropout layer if there is an evidence of overfitting in your findings\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "num_classes = 9\n",
        "model = Sequential([ data_augument,\n",
        "                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n",
        "      \n",
        "])\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu', input_shape = (180, 180, 32)))\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation = \"softmax\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-bfL45IR5TT"
      },
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fsXZV8JR68h"
      },
      "outputs": [],
      "source": [
        "### Your code goes here\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdyNE1Q7R-vC"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oyzd4XegSAKA",
        "outputId": "7477a5ae-624c-43fe-cb27-3bd7afd32705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "56/56 [==============================] - 503s 9s/step - loss: 2.0701 - accuracy: 0.1763 - val_loss: 2.0282 - val_accuracy: 0.1946\n",
            "Epoch 2/10\n",
            "56/56 [==============================] - 508s 9s/step - loss: 2.0314 - accuracy: 0.2093 - val_loss: 2.0232 - val_accuracy: 0.2058\n",
            "Epoch 3/10\n",
            "56/56 [==============================] - 507s 9s/step - loss: 2.0182 - accuracy: 0.1959 - val_loss: 2.0419 - val_accuracy: 0.1879\n",
            "Epoch 4/10\n",
            "56/56 [==============================] - 508s 9s/step - loss: 2.0216 - accuracy: 0.1953 - val_loss: 2.0204 - val_accuracy: 0.1924\n",
            "Epoch 5/10\n",
            "31/56 [===============>..............] - ETA: 3:35 - loss: 1.9981 - accuracy: 0.2067"
          ]
        }
      ],
      "source": [
        "## Your code goes here, note: train your model for 20 epochs\n",
        "epochs=30\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcIUp1DvSEIr"
      },
      "source": [
        "### Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vQjOFk8SFbX"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJN5gGA_SJyS"
      },
      "source": [
        "### Todo: Write your findings after the model fit, see if there is an evidence of model overfit or underfit. Do you think there is some improvement now as compared to the previous model run?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v8q6s02SNk8"
      },
      "source": [
        "Finding from Second Model\n",
        "\n",
        "There is no improvement in accuracy but we can definitely see the overfitting problem has solved due to data augmentation\n",
        "\n",
        "We can increase the epochs to increase the accuracy so it's too early for judgement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQdrKN0aSQTa"
      },
      "source": [
        "### Todo: Find the distribution of classes in the training dataset.\n",
        "### Context: Many times real life datasets can have class imbalance, one class can have proportionately higher number of samples compared to the others. Class imbalance can have a detrimental effect on the final model quality. Hence as a sanity check it becomes important to check what is the distribution of classes in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEjx-OcPSKyn"
      },
      "outputs": [],
      "source": [
        "def class_distribution_count(directory):\n",
        "    \n",
        "    #count number of image in each classes\n",
        "    count= []\n",
        "    for path in pathlib.Path(directory).iterdir():\n",
        "        if path.is_dir():\n",
        "            count.append(len([name for name in os.listdir(path)\n",
        "                               if os.path.isfile(os.path.join(path, name))]))\n",
        "    \n",
        "    #name of the classes\n",
        "    sub_directory = [name for name in os.listdir(directory)\n",
        "                    if os.path.isdir(os.path.join(directory, name))]\n",
        "    \n",
        "    #return dataframe with image count and class.\n",
        "    return pd.DataFrame(list(zip(sub_directory,count)),columns =['Class', 'No. of Image'])\n",
        "\n",
        "df = class_distribution_count(data_dir_train)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g72AMRc-SZBr"
      },
      "outputs": [],
      "source": [
        "dataframe_dict_original = dict(zip(path_list, lesion_list))\n",
        "original_df = pd.DataFrame(list(dataframe_dict_original.items()),columns = ['Path','Label'])\n",
        "original_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8T__ZoAESazS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "f106593d-1d24-476b-8384-f9a73d09f77e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f75c68ae110>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#Visualize the Number of image in each class.\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x=\"No. of Image\", y=\"Class\", data=df,\n",
        "            label=\"Class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB6oC3-yVYzb"
      },
      "source": [
        "#### To use Augmentor, the following general procedure is followed:\n",
        "\n",
        "Instantiate a Pipeline object pointing to a directory containing your initial image data set.\n",
        "Define a number of operations to perform on this data set using your Pipeline object.\n",
        "Execute these operations by calling the Pipeline’s sample() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uLFiEWCfU2g2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61778d91-66a5-4d8b-86db-6ef6579ec602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Augmentor\n",
            "  Downloading Augmentor-0.2.10-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (1.21.6)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (7.1.2)\n",
            "Requirement already satisfied: tqdm>=4.9.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (4.64.0)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (0.16.0)\n",
            "Installing collected packages: Augmentor\n",
            "Successfully installed Augmentor-0.2.10\n"
          ]
        }
      ],
      "source": [
        "#install Augmentor\n",
        "!pip install Augmentor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "SEr6YrKhU7jw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e83883-2c75-4888-c713-cc17b8087a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised with 114 image(s) found.\n",
            "Output directory set to /content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train/actinic keratosis/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7F75C1874050>: 100%|██████████| 500/500 [00:24<00:00, 20.07 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised with 376 image(s) found.\n",
            "Output directory set to /content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train/basal cell carcinoma/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7F75C67FBFD0>: 100%|██████████| 500/500 [00:22<00:00, 22.66 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised with 95 image(s) found.\n",
            "Output directory set to /content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train/dermatofibroma/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7F75C52B5090>: 100%|██████████| 500/500 [00:24<00:00, 20.65 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised with 438 image(s) found.\n",
            "Output directory set to /content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train/melanoma/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=1024x768 at 0x7F75C1894510>: 100%|██████████| 500/500 [01:58<00:00,  4.23 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised with 357 image(s) found.\n",
            "Output directory set to /content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train/nevus/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=1504x1129 at 0x7F75C1874E50>: 100%|██████████| 500/500 [01:43<00:00,  4.83 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised with 462 image(s) found.\n",
            "Output directory set to /content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train/pigmented benign keratosis/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7F75C1874B50>: 100%|██████████| 500/500 [00:22<00:00, 22.66 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised with 77 image(s) found.\n",
            "Output directory set to /content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train/seborrheic keratosis/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x768 at 0x7F75C18DC710>: 100%|██████████| 500/500 [00:49<00:00, 10.04 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised with 181 image(s) found.\n",
            "Output directory set to /content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train/squamous cell carcinoma/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7F75C186E610>: 100%|██████████| 500/500 [00:24<00:00, 20.55 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised with 139 image(s) found.\n",
            "Output directory set to /content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train/vascular lesion/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=600x450 at 0x7F75C16415D0>: 100%|██████████| 500/500 [00:22<00:00, 22.38 Samples/s]\n"
          ]
        }
      ],
      "source": [
        "path_to_training_dataset=\"/content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\"\n",
        "\n",
        "import Augmentor\n",
        "for i in class_names:\n",
        "    p = Augmentor.Pipeline(path_to_training_dataset + i)\n",
        "    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
        "    p.sample(500)  #Adding 500 samples per class to make sure that none of the classes are sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "sX5mW8rXVgvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a158cf16-bed4-4ec4-c00d-fe5e68968f33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4500\n"
          ]
        }
      ],
      "source": [
        "#Count total number of image generated by Augmentor.\n",
        "data_dir_train1 = pathlib.Path(\"/content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\")\n",
        "image_count_train = len(list(data_dir_train1.glob('*/output/*.jpg')))\n",
        "#image_count_train1 = len(list(data_dir_train1.glob('*/*.jpg')))\n",
        "print(image_count_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "emkObsSLXQLi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "817db63a-449e-4647-f235-738313cbc34d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   Path              Label\n",
              "0     /content/gdrive/MyDrive/CNNImages/Skin cancer ...  actinic keratosis\n",
              "1     /content/gdrive/MyDrive/CNNImages/Skin cancer ...  actinic keratosis\n",
              "2     /content/gdrive/MyDrive/CNNImages/Skin cancer ...  actinic keratosis\n",
              "3     /content/gdrive/MyDrive/CNNImages/Skin cancer ...  actinic keratosis\n",
              "4     /content/gdrive/MyDrive/CNNImages/Skin cancer ...  actinic keratosis\n",
              "...                                                 ...                ...\n",
              "2234  /content/gdrive/MyDrive/CNNImages/Skin cancer ...    vascular lesion\n",
              "2235  /content/gdrive/MyDrive/CNNImages/Skin cancer ...    vascular lesion\n",
              "2236  /content/gdrive/MyDrive/CNNImages/Skin cancer ...    vascular lesion\n",
              "2237  /content/gdrive/MyDrive/CNNImages/Skin cancer ...    vascular lesion\n",
              "2238  /content/gdrive/MyDrive/CNNImages/Skin cancer ...    vascular lesion\n",
              "\n",
              "[2239 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5f741e00-a537-46bf-b743-ef1d4f57980b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Path</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/gdrive/MyDrive/CNNImages/Skin cancer ...</td>\n",
              "      <td>actinic keratosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/gdrive/MyDrive/CNNImages/Skin cancer ...</td>\n",
              "      <td>actinic keratosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/gdrive/MyDrive/CNNImages/Skin cancer ...</td>\n",
              "      <td>actinic keratosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/gdrive/MyDrive/CNNImages/Skin cancer ...</td>\n",
              "      <td>actinic keratosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/gdrive/MyDrive/CNNImages/Skin cancer ...</td>\n",
              "      <td>actinic keratosis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2234</th>\n",
              "      <td>/content/gdrive/MyDrive/CNNImages/Skin cancer ...</td>\n",
              "      <td>vascular lesion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2235</th>\n",
              "      <td>/content/gdrive/MyDrive/CNNImages/Skin cancer ...</td>\n",
              "      <td>vascular lesion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2236</th>\n",
              "      <td>/content/gdrive/MyDrive/CNNImages/Skin cancer ...</td>\n",
              "      <td>vascular lesion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2237</th>\n",
              "      <td>/content/gdrive/MyDrive/CNNImages/Skin cancer ...</td>\n",
              "      <td>vascular lesion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2238</th>\n",
              "      <td>/content/gdrive/MyDrive/CNNImages/Skin cancer ...</td>\n",
              "      <td>vascular lesion</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2239 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f741e00-a537-46bf-b743-ef1d4f57980b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5f741e00-a537-46bf-b743-ef1d4f57980b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5f741e00-a537-46bf-b743-ef1d4f57980b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "##\n",
        "path_list=[]\n",
        "lesion_list=[]\n",
        "for i in class_names:\n",
        "      \n",
        "    for j in data_dir_train1.glob(i+'/*.jpg'):\n",
        "        path_list.append(str(j))\n",
        "        lesion_list.append(i)\n",
        "dataframe_dict_original = dict(zip(path_list, lesion_list))\n",
        "new_df = pd.DataFrame(list(dataframe_dict_original.items()),columns = ['Path','Label'])\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNRx5uxdXTIw"
      },
      "source": [
        "### Lets see the distribution of augmented data after adding new images to the original training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "TmDa4M_PXUMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f95081c2-8b21-4c57-b59b-fda7ed4cafc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pigmented benign keratosis    462\n",
              "melanoma                      438\n",
              "basal cell carcinoma          376\n",
              "nevus                         357\n",
              "squamous cell carcinoma       181\n",
              "vascular lesion               139\n",
              "actinic keratosis             114\n",
              "dermatofibroma                 95\n",
              "seborrheic keratosis           77\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "new_df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuyu7d32XiTF"
      },
      "source": [
        "### Todo: Train the model on the data created using Augmentor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "PDt1VJmHXjsF"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "dGKUe0VNXmAQ"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "data_dir_train1=pathlib.Path(\"/content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KQ8tO8ktXoPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ac858c-2f1d-4f43-d5e7-3f75c6fc8a26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "data_dir_train1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9QzvwMvXrA5"
      },
      "source": [
        "### Todo: Create a training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "km69ihDzXsB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d72dfe07-3a6f-43fb-8775-78959cf3e54d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2239\n"
          ]
        }
      ],
      "source": [
        "image_count_train1 = len(list(data_dir_train1.glob('*/*.jpg')))\n",
        "print(image_count_train1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "REQ-xR3mXv4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b67ebb-25a7-4cb3-cad6-0133a16d6949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6739 files belonging to 9 classes.\n",
            "Using 5392 files for training.\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "data_dir_train1=pathlib.Path(\"/content/gdrive/MyDrive/CNNImages/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\")\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir_train1,\n",
        "  seed=123,\n",
        "  validation_split = 0.2,\n",
        "  subset = \"training\",\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7BXVfJNXyHH"
      },
      "source": [
        "### Todo: Create a validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Pm9cr6SIXzMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9188671a-9a8b-4297-97bf-03905b9cfc8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6739 files belonging to 9 classes.\n",
            "Using 1347 files for validation.\n"
          ]
        }
      ],
      "source": [
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir_train1,\n",
        "  seed=123,\n",
        "  validation_split = 0.2,\n",
        "  subset = 'validation',\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIPUK5hEX3du"
      },
      "source": [
        "### Todo: Create your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "DRUTQ9-5X6NU"
      },
      "outputs": [],
      "source": [
        "## You can use Dropout layer if there is an evidence of overfitting in your findings\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "num_classes = 9\n",
        "model = Sequential([ \n",
        "                    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width,3))\n",
        "      \n",
        "])\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu', input_shape = (180, 180, 32)))\n",
        "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation = \"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqgshmcxX90P"
      },
      "source": [
        "### Todo: Compile your model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Jo2pCCzvX_JM"
      },
      "outputs": [],
      "source": [
        "## ### Your code goes here\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWpWg9l0YBrB"
      },
      "source": [
        "### Todo: Train your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwzRiqnfYGN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13823f53-6471-4d05-ce16-bc4cc10ee8e7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "169/169 [==============================] - 1503s 9s/step - loss: 2.1795 - accuracy: 0.1447 - val_loss: 2.8131 - val_accuracy: 0.1321\n",
            "Epoch 2/50\n",
            "169/169 [==============================] - 1501s 9s/step - loss: 2.1421 - accuracy: 0.1732 - val_loss: 1.9130 - val_accuracy: 0.2791\n",
            "Epoch 3/50\n",
            "169/169 [==============================] - 1499s 9s/step - loss: 1.8345 - accuracy: 0.2908 - val_loss: 1.6767 - val_accuracy: 0.3682\n",
            "Epoch 4/50\n",
            "169/169 [==============================] - 1499s 9s/step - loss: 1.6656 - accuracy: 0.3655 - val_loss: 1.6550 - val_accuracy: 0.3682\n",
            "Epoch 5/50\n",
            "169/169 [==============================] - 1502s 9s/step - loss: 1.5538 - accuracy: 0.3976 - val_loss: 1.5774 - val_accuracy: 0.4269\n",
            "Epoch 6/50\n",
            "169/169 [==============================] - 1500s 9s/step - loss: 1.4294 - accuracy: 0.4642 - val_loss: 1.4281 - val_accuracy: 0.4796\n",
            "Epoch 7/50\n",
            "169/169 [==============================] - 1491s 9s/step - loss: 1.2139 - accuracy: 0.5568 - val_loss: 1.3758 - val_accuracy: 0.4944\n",
            "Epoch 8/50\n",
            "169/169 [==============================] - ETA: 0s - loss: 1.0816 - accuracy: 0.5979"
          ]
        }
      ],
      "source": [
        "epochs =50\n",
        "## Your code goes here, use 50 epochs.\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdffnAdOYEEv"
      },
      "source": [
        "### Todo: Visualize the model results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bWIRjBzYLAk"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPdWCDuvYO9O"
      },
      "source": [
        "### Todo: Analyze your results here. Did you get rid of underfitting/overfitting? Did class rebalance help?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgfC2_0rYSbv"
      },
      "source": [
        "Accuracy on training data has increased by using Augmentor library\n",
        "\n",
        "Model is still overfitting\n",
        "\n",
        "The problem of overfitting can be solved by add more layer,neurons or adding dropout layers.\n",
        "\n",
        "The Model can be further improved by tuning the hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIrYaN9WYP5c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Melanoma_assignment_Dwaraganathan_DK_16thAug_4PM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPkGMXVe9oNnpkpJHsW3Vk6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}